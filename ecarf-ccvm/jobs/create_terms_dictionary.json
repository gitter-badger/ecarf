{
    "id":"ecarf-forward-reasoning-one-query-job",
    "description": "Forward reasoning using BigQuery where all the reasoning queries are combined into one",
    "mode":"Serial",
    "data":{
        "sourceBucket": "swetodblp1",
        "bucket": "swetodblp1-work",
        "schema": "opus_august2007_closure.nt",
        "table": "ontologies.swetodblp",
        "fakeItems": ["item1", "item2"],
        "numberOfProcessors": 2
    },
    "vmConfig": {
        "zoneId": "us-central1-a",
        "imageId": "ecarf-1000/global/images/ecarf-centos-6-v20160126-1",
        "vmType": "n1-standard-2",
        "networkId": "default",
        "diskType": "pd-standard",
        "startupScript": "su - omerio -c 'cd /home/omerio/ecarf && git pull && /home/omerio/maven/bin/mvn -q clean compile install -Dmaven.test.skip=true && cd /home/omerio/ecarf/ecarf-evm && export VM_XMS=\"-Xms512m\" VM_XMX=\"-Xmx6g\" && mvn -q exec:exec 2>&1 & exit 0' exit 0"  
    },
    "tasks":[
        {
            "id":"start-processors-task",
            "description": "A dummy task to start all the processors before hand",
            "className":"io.ecarf.core.cloud.task.processor.DummyProcessorTask",
            "target":"PROCESSOR",
            "input":{
                "item":"#fakeItems"
            },
            "partitioning":{
                "type":"ITEMS",
                "input":{
                    "items":"#fakeItems"
                }
            },
            "errorAction":"EXIT"
        },
        {
            "id":"schema-term-count-task",
            "description": "Load the schema and do a count of the relevant terms",
            "className":"io.ecarf.core.cloud.task.coordinator.CountSchemaTermTask",
            "target":"COORDINATOR",
            "input":{
                "sourceBucket":"#sourceBucket",
                "bucket":"#bucket",
                "schemaFile":"#schema"
            },
            "output":[
                "schemaTermsFile"
            ],
            "errorAction":"EXIT"
        },
        {
            "id":"partition-load-task",
            "description": "Find all the files in the cloud bucket and add them to a list of items",
            "className":"io.ecarf.core.cloud.task.coordinator.CreateFileItemsTask",
            "target":"COORDINATOR",
            "input":{
                "bucket":"#sourceBucket"
            },
            "output":[
                "fileItems"
            ],
            "errorAction":"EXIT"
        },
        {
            "id":"extract-count-terms-task",
            "description": "Extract all the term and count the relevant schema terms",
            "className":"io.ecarf.core.cloud.task.processor.analyze.ExtractAndCountTermsTask",
            "target":"PROCESSOR",
            "input":{
                "sourceBucket":"#sourceBucket",
                "bucket":"#bucket",
                "schemaTermsFile":"#schemaTermsFile",
                "files":"#filePartitions"
            },
            "partitioning":{
                "type":"FUNCTION",
                "functionName":"BinPackingPartition",
                "input":{
                    "items":"#fileItems",
                    "numberOfBins":"#numberOfProcessors"
                },
                "output":"filePartitions"
            },
            "errorAction":"EXIT"
        },
        {
            "id":"create-dictionary-task",
            "description": "Join up all the terms extracted by all the processors and join them into a dictionary",
            "className":"io.ecarf.core.cloud.task.common.CreateTermDictionaryTask",
            "target":"COORDINATOR",
            "input":{
                "processors":"#processors",
                "sourceBucket":"#sourceBucket",
                "bucket":"#bucket",
                "schemaFile":"#schema"
            },
            "output":[
                "dictionary"
            ],
            "errorAction":"EXIT"
        },
        {
            "id":"processor-do-upload-logs-task",
            "description": "Distribute the uploading of logs task between the various processors, each processor then uploads its own logs to cloud storage",
            "className":"io.ecarf.core.cloud.task.common.DoUploadOutputLogTask",
            "target":"PROCESSOR",
            "input":{
                "bucket":"#bucket"
            },
            "partitioning":{
                "type":"ITEMS",
                "input":{
                    "items":"#processors"
                }
            },
            "errorAction":"EXIT"
        },
        {
            "id":"coordinator-do-upload-logs-task",
            "description": "The coordinator to upload its own logs to cloud storage",
            "className":"io.ecarf.core.cloud.task.common.DoUploadOutputLogTask",
            "target":"COORDINATOR",
            "input":{
                "bucket":"#bucket"
            },
            "errorAction":"EXIT"
        }
    ]
}