{
    "id":"dbpedia-count-productive-triples-job",
    "description": "Count all the productive instance triples for DBPedia",
    "mode":"Serial",
    "data":{
        "sourceBucket": "dbpedia",
        "bucket": "dbpedia-part-type",
        "schema": "dbpedia_3.9_gen_closure.nt",
        "countOnly": "true",
	"jobId": "csize",
	"fileMatch":"",
        "maxSize": 0,
        "fakeItems": ["item1", "item2", "item3", "item4", "item1", "item2", "item3", "item4"],
        "numberOfProcessors": 8 
    },
    "vmConfig": {
        "zoneId": "us-central1-a",
        "imageId": "ecarf-1000/global/images/ecarf-centos-6-v20160126-2",
        "vmType": "n1-standard-2",
        "networkId": "default",
        "diskType": "pd-standard",
        "startupScript": "su - omerio -c 'cd /home/omerio/ecarf/ecarf-evm && export VM_XMS=\"-Xms512m\" VM_XMX=\"-Xmx6g\" && mvn -q exec:exec 2>&1 & exit 0' exit 0"  
    },
    "tasks":[
        {
            "id":"schema-term-count-task",
            "description": "Load the schema and do a count of the relevant terms",
            "className":"io.ecarf.core.cloud.task.coordinator.CountSchemaTermTask",
            "target":"COORDINATOR",
            "input":{
                "sourceBucket":"#sourceBucket",
                "bucket":"#bucket",
                "schemaFile":"#schema"
            },
            "output":[
                "schemaTermsFile"
            ],
            "errorAction":"EXIT"
        },
        {
            "id":"partition-load-task",
            "description": "Find all the files in the cloud bucket and add them to a list of items",
            "className":"io.ecarf.core.cloud.task.coordinator.CreateFileItemsTask",
            "target":"COORDINATOR",
            "input":{
                "bucket":"#sourceBucket",
		"fileMatch":"#fileMatch",
		"maxSize":"#maxSize"
            },
            "output":[
                "fileItems"
            ],
            "errorAction":"EXIT"
        },
        {
            "id":"start-processors-task",
            "description": "A dummy task to start all the processors before hand",
            "className":"io.ecarf.core.cloud.task.processor.DummyProcessorTask",
            "target":"PROCESSOR",
            "input":{
                "item":"#fakeItems"
            },
            "partitioning":{
                "type":"ITEMS",
                "input":{
                    "items":"#fakeItems"
                }
            },
            "errorAction":"EXIT"
        },
        {
            "id":"process-load-task",
            "description": "Distribute the load task between the various processors, each processor then processes the files assigned to it",
            "className":"io.ecarf.core.cloud.task.processor.ProcessLoadTask",
            "target":"PROCESSOR",
            "input":{
                "sourceBucket":"#sourceBucket",
                "bucket":"#bucket",
                "countOnly":"#countOnly",
                "schemaTermsFile":"#schemaTermsFile",
                "files":"#filePartitions"
            },
            "partitioning":{
                "type":"FUNCTION",
                "functionName":"BinPackingPartition",
                "input":{
                    "items":"#fileItems",
                    "numberOfBins":"#numberOfProcessors"
                },
                "output":"filePartitions"
            },
            "errorAction":"EXIT"
        },
        {
            "id":"term-stats-task",
            "description": "Read and join up the term stats generated by the various processors",
            "className":"io.ecarf.core.cloud.task.coordinator.CombineTermStatsTask",
            "target":"COORDINATOR",
            "input":{
                "processors":"#processors",
                "bucket":"#bucket"
            },
            "output":[
                "termItems"
            ],
            "errorAction":"EXIT"
        },
        {
            "id":"processor-do-upload-logs-task",
            "description": "Distribute the uploading of logs task between the various processors, each processor then uploads its own logs to cloud storage",
            "className":"io.ecarf.core.cloud.task.common.DoUploadOutputLogTask",
            "target":"PROCESSOR",
            "input":{
                "bucket":"#bucket",
                "jobId":"#jobId"
            },
            "partitioning":{
                "type":"ITEMS",
                "input":{
                    "items":"#processors"
                }
            },
            "errorAction":"EXIT"
        },
        {
            "id":"coordinator-do-upload-logs-task",
            "description": "The coordinator to upload its own logs to cloud storage",
            "className":"io.ecarf.core.cloud.task.common.DoUploadOutputLogTask",
            "target":"COORDINATOR",
            "input":{
                "bucket":"#bucket",
                "jobId":"#jobId"
            },
            "errorAction":"EXIT"
        }
    ]
}
