{
    "id":"ecarf-forward-reasoning-swetodblp",
    "description": "Forward reasoning on the Swetodblp datasets using dictionary compression",
    "mode":"Serial",
    "data":{
        "sourceBucket":"swetodblp1",
        "bucket":"swetodblp-fullrun-1",
        "schema":"opus_august2007_closure.nt",
        "table":"ontologies.swetodblp",
        "termStatsFile":"term_stats.json",
        "encodedSchemaFile":"opus_august2007_closure_encoded.csv",
        "encodedTermStatsFile":"term_stats_encoded.json",
        "dictionaryFile":"swetodblp_dictionary.kryo.gz",
        "countOnly": "false",
        "encode": "true",
        "jobId":"run-1",
        "splitLocation": "-1",
        "numberOfProcessors": 2 
    },
    "vmConfig": {
        "zoneId": "us-central1-a",
        "imageId": "ecarf-1000/global/images/ecarf-centos-6-v20160126-2",
        "vmType": "n1-standard-2",
        "cost": 0.100,
        "minUsage": 600,
        "networkId": "default",
        "diskType": "pd-standard",
        "startupScript":"su - omerio -c 'cd /home/omerio/ecarf && git pull && /home/omerio/maven/bin/mvn -q clean compile install -Dmaven.test.skip=true && cd /home/omerio/ecarf/ecarf-evm && export VM_XMS=\"-Xms512m\" VM_XMX=\"-Xmx6g\" && mvn -q exec:exec 2>&1 & exit 0' exit 0"  
    },
    "tasks":[
        {
            "id":"schema-term-extract-task",
            "description": "Load the schema and extract the relevant the relevant terms",
            "className":"io.ecarf.core.cloud.task.coordinator.CountSchemaTermTask",
            "target":"COORDINATOR",
            "input":{
                "sourceBucket":"#sourceBucket",
                "bucket":"#bucket",
                "schemaFile":"#schema"
            },
            "output":[
                "schemaTermsFile"
            ],
            "errorAction":"EXIT"
        },
        {
            "id":"partition-load-task",
            "description": "Find all the files in the cloud bucket and add them to a list of items",
            "className":"io.ecarf.core.cloud.task.coordinator.CreateFileItemsTask",
            "target":"COORDINATOR",
            "input":{
                "bucket":"#sourceBucket"
            },
            "output":[
                "fileItems"
            ],
            "errorAction":"EXIT"
        },
        {
            "id":"start-processors-task",
            "description": "A dummy task to start all the processors before hand",
            "className":"io.ecarf.core.cloud.task.processor.DummyProcessorTask",
            "target":"PROCESSOR",
            "partitioning":{
                "type":"COUNT",
                "countRef": "#numberOfProcessors" 
            },
            "errorAction":"EXIT"
        },
        {
            "id":"extract-count-terms-task",
            "description": "Extract all the term and count the relevant schema terms",
            "className":"io.ecarf.core.cloud.task.processor.analyze.ExtractCountTerms2PartTask",
            "target":"PROCESSOR",
            "input":{
                "sourceBucket":"#sourceBucket",
                "bucket":"#bucket",
                "schemaTermsFile":"#schemaTermsFile",
                "splitLocation":"#splitLocation",
                "files":"#filePartitions"
            },
            "partitioning":{
                "type":"FUNCTION",
                "functionName":"BinPackingPartition",
                "input":{
                    "items":"#fileItems",
                    "numberOfBins":"#numberOfProcessors"
                },
                "output":"filePartitions"
            },
            "errorAction":"EXIT"
        },
        {
            "id":"term-stats-task",
            "description": "Read and join up the term stats generated by the various processors",
            "className":"io.ecarf.core.cloud.task.coordinator.CombineTermStatsTask",
            "target":"COORDINATOR",
            "input":{
                "processors":"#processors",
                "bucket":"#bucket",
                "termStatsFile":"#termStatsFile"
            },
            "output":[
                "termItems"
            ],
            "errorAction":"EXIT"
        },
        {
            "id":"assemble-dictionary-task",
            "description":"Assemble the terms dictionary",
            "className":"io.ecarf.core.cloud.task.processor.dictionary.AssembleDictionaryTask",
            "target":"PROCESSOR",
            "input":{
                "bucket":"#bucket",
                "targetBucket":"#bucket",
                "schemaBucket":"#sourceBucket",
                "schemaFile":"#schema",
                "termStatsFile":"#termStatsFile",
                "encodedSchemaFile":"#encodedSchemaFile",
                "encodedTermStatsFile":"#encodedTermStatsFile",
                "dictionaryFile":"#dictionaryFile"
            },
            "partitioning":{
                "type":"COUNT",
                "count": 1
            },
            "errorAction":"EXIT"
        },
        {
            "id":"process-load-task",
            "description": "Distribute the load task between the various processors, each processor then processes the files assigned to it",
            "className":"io.ecarf.core.cloud.task.processor.ProcessLoadTask",
            "target":"PROCESSOR",
            "input":{
                "sourceBucket":"#sourceBucket",
                "bucket":"#bucket",
                "countOnly":"#countOnly",
                "encode":"#encode",
                "dictionaryFile":"#dictionaryFile",
                "schemaTermsFile":"#schemaTermsFile",
                "files":"#filePartitions"
            },
            "partitioning":{
                "type":"FUNCTION",
                "functionName":"BinPackingPartition",
                "input":{
                    "items":"#fileItems",
                    "numberOfBins":"#numberOfProcessors"
                },
                "output":"filePartitions"
            },
            "errorAction":"EXIT"
        },
        {
            "id":"big-data-load-task",
            "description": "Load all the files processed by the processors into BigQuery",
            "className":"io.ecarf.core.cloud.task.coordinator.LoadBigDataFilesTask",
            "target":"COORDINATOR",
            "input":{
                "table":"#table",
                "bucket":"#bucket",
                "encode":"#encode"
            },
            "errorAction":"EXIT"
        },
        {
            "id":"process-reason-task",
            "description": "Distribute the reason task between the various processors, each processor then reasons over the terms assigned to it",
            "className":"io.ecarf.core.cloud.task.processor.reason.phase2.DoReasonTask6",
            "target":"PROCESSOR",
            "input":{
                "bucket":"#bucket",
                "table":"#table",
                "schemaFile":"#schema",
                "terms":"#termPartitions"
            },
            "partitioning":{
                "type":"FUNCTION",
                "functionName":"BinPackingPartition",
                "input":{
                    "items":"#termItems",
                    "numberOfBins":"#numberOfProcessors"
                },
                "output":"termPartitions"
            },
            "errorAction":"EXIT"
        },
        {
            "id":"processor-do-upload-logs-task",
            "description": "Distribute the uploading of logs task between the various processors, each processor then uploads its own logs to cloud storage",
            "className":"io.ecarf.core.cloud.task.common.DoUploadOutputLogTask",
            "target":"PROCESSOR",
            "input":{
                "bucket":"#bucket",
                "jobId":"#jobId"
            },
            "partitioning":{
                "type":"ITEMS",
                "input":{
                    "items":"#processors"
                }
            },
            "errorAction":"CONTINUE"
        },
        {
            "id":"coordinator-do-upload-logs-task",
            "description": "The coordinator to upload its own logs to cloud storage",
            "className":"io.ecarf.core.cloud.task.common.DoUploadOutputLogTask",
            "target":"COORDINATOR",
            "input":{
                "bucket":"#bucket",
                "jobId":"#jobId"
            },
            "errorAction":"CONTINUE"
        }
    ]
}
